%% bare_conf.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

\documentclass[conference]{IEEEtran}

\usepackage{graphicx, url, tikz}
\usepackage{standalone}
\usepackage{amsmath,bm,times,amssymb}
\usepackage{mathtools}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{subcaption}
\usepackage{algorithm,algorithmic}

\usetikzlibrary{datavisualization}
\usetikzlibrary{shapes,arrows,shadows}
\usetikzlibrary{datavisualization.formats.functions}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\RequirePackage[singlelinecheck=off]{caption}

%%%%
\definecolor{mygreen}{RGB}{117,167,117}
\definecolor{myred}{RGB}{255,1,1}
\newcommand\myent[1]{%
  \footnotesize%
  $#1$
}
\colorlet{negro}{black}
\colorlet{gris}{black!70}
\colorlet{rojo}{red!70!black}
\colorlet{rojol}{red}
%%%%

\newtheorem{condition}{Condition}
\newtheorem{assumption}{Assumption}
\newtheorem{colloary}{Colloary}
\newtheorem{theorem}{\bf Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{notation}{Notation}
\newtheorem{definition}{\bf Definition}
\newtheorem{remark}{Remark}

\begin{document}
\title{EasiCrawl: A Sleep-aware Schedule Method \\for Crawling IoT Sensors}

% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{
\IEEEauthorblockN{
	Li Meng\IEEEauthorrefmark{1}\IEEEauthorrefmark{2}, 
	Haiming Chen\IEEEauthorrefmark{1}, 
	Cui Li\IEEEauthorrefmark{1}}
\IEEEauthorblockA{\IEEEauthorrefmark{1}Institute of Computing Technology, Chinese Academy of Sciences}
\IEEEauthorblockA{\IEEEauthorrefmark{2}University of Chinese Academy of Sciences, China}
\IEEEauthorblockA{\{limeng, chenhaiming, lcui\}@ict.ac.cn}
}

% make the title area
\maketitle


\begin{abstract}
	
How to search the Internet of things effectively to identify available and useful low power sensors is an important issue for constructing IoT applications. 
Crawling the content of low power IoT sensors is a fundamental step towards building a generic IoT search engine.
However, crawling IoT sensors is very different from crawling web. 
In IoT systems, many low-power sensors may sleep periodically, which may result in invalid crawls and unpredictable latency. 
Besides, the energy consumption of crawl access is non-negligible due to the limited battery life.
As a consequence, the traditional crawling schedule strategy is not suitable for IoT search engine anymore.
In this work, we first formulated the problem of crawling periodically sleeping sensors as a constrained schedule optimization problem, with the objective of minimized latency. 
Then, we proposed an a sleep-aware heuristic schedule, named EasiCrawl to get the near optimal expected latency. 
Next, we evaluate EasiCrawl with simulations which show advantages over other greedy-based methods.
Finally a case study is performed with real world data from Xively, showing that EasiCrawl has lower crawling latency than traditional Web-based search scheduling, and is an effective sleeping-aware schedule method for crawling IoT sensors.

\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction} \label{introduction}

A large number of sensors have been deployed to collect environmental information and connected to the Internet, which assists the human society to sense and control the physical world in an intelligent way. 
The extended Internet with sensors are known as the Internet of Things (IoT). 
With the booming of public access sensors in IoT, it is an inevitable trend to develop a generic search engine for IoT users and application builders to find different kinds of IoT sensors as required. 
It is expected that a generic IoT search engine can be used in a variety of scenes to improve the possibility of sharing sensors with others.
In this way, the redundancy of sensor deployment and facilitate construction of different IoT applications can be significantly reduced.
% More approperate examples needed
More specifically, in smart city applications, public accessible sensors may be found in an ad hoc manner to provide predications relevant to traffic jam and queuing status. 
Smart home applications need searchable sensors and devices to detect human activities and provide real-time sensing of the indoor environment. 
In recent future, unmanned systems like automatic driving is likely to be heavily dependent on information gathered from nearby sensors or other IoT sensors, which also needs effective sensor search. 

% Practicability 
Existing works show that IoT search engine can be realized by traditional Web search engines with the help of standardized protocols and semantic descriptions of sensors\cite{Pfisterer2011}. 
Here, the descriptions crawled by IoT search engines include both static contents and dynamic records generated by the sensor which is described by a dynamic description method.
There are many research group are currently working on the standard of dynamic describing methods, such as CoRE\cite{CoREWorkingGroup2012} from IETF and SensorML\cite{botts2007opengis} from the OGC. 
With the description methods, IoT search system can be designed with technicals in traditional web search.
Fig.\ref{fig:architecture} shows the architecture of such an IoT search system.
It consists of events, sensors and a IoT search engine.
In this search system, it is supposed that all sensors support IP protocol and are treated as web pages whose content is automatically updating.
IoT crawlers crawl these descriptions files, and copy them into a local repository at server side. 
The repository is an organized storage, with which the user queries about IoT sensors can be accomplished.
It is assumed that the users only interest in the events monitored by the corresponded sensors, which arrives periodically or just randomly. 
With the help of semantic IoT method, IoT sensors first capture the event stream and then continuously store these events within their description files. 
IoT search engine regularly accesses these IoT descriptions and updates the related items in the repository accordingly. 

\begin{figure}
	\vspace{0.5em}
	\centering
	\hspace{-3.0em}
	\includestandalone[width=\linewidth]{pic/architecture}
	\captionsetup{justification=centering}
	\caption{Architecture of IoT Search System}
	\vspace{-1.0em}
	\label{fig:architecture}
\end{figure}

% Challenge of IoT search
However, even with dynamic description, the design of IoT search engine is not a trivial task. 
The main challenge is induced from the instinctive characteristics of IoT sensors. 
Firstly and the main concern of this paper, IoT sensors behave differently with their particular sleep mode, whereas freshness of descriptions crawled is a critical measurement of search. 
If a sensor is sleeping, it can neither capture events nor be visited by IoT crawlers. 
If information are not collected in a right schedule, the freshness of query result will be impacted significantly.
We use a clip of real world data to illustrate how the instinct of sensors' sleeping impact the crawl quality of IoT search system.
Fig.\ref{fig:smarthome} is a data stream from a temperature sensor in a smart home application, which acts as a sensor for air-conditioner control. 
The quick variations and violation of threshold are taken as \textbf{interesting event}, which is logged into the description file immediately by the sensor.
Suppose the users are not at home, these sensors will go into sleep periodically to save power.
We marked all these interesting events out in the figure as circles.
The traditional crawl scheduling method perform not so well in this case, due to lacking awareness of sensors' sleep. 
Traditional scheduler uses a periodical crawl method to retrieve sensors' description files.
However, it will cause invalid accesses to sleeping ones, like those showed at certain times in Fig.\ref{fig:smarthome} with green crosses. 
Those missed readings leads to a delay across the whole sensor' sleeping cycle. 
At time 210, latency may also be accumulated to a significantly high level. 
Secondly, most of IoT sensors are low-power device, where the energy consumption is a critical factor. 
An unnecessary high frequency of visiting a sensor may quickly exhaust sensor's energy.
In this paper, we propose a new server-side scheduling method, named EasiCrawl, which can proceed sleep-aware search by taking these characteristics of IoT sensors into consideration.

\begin{figure}
	\centering
	\hspace{-3.0em}
	\includestandalone[width=\linewidth]{pic/smarthome}
	\captionsetup{justification=centering}
	\caption{Readings of a Temperature Sensor\\ in a Smart Home Application}
	\vspace{-1.0em}
	\label{fig:smarthome}
\end{figure}

The main contributions of this work are:
(1) Although crawler scheduling is well studied, to the best of our knowledge, this paper is the first to discuss crawling of IoT consisting of sensors with sleep character and energy constraints.
(2) An iterative optimization approach is proposed for crawling sensors with different sleep pattern. 
The approach includes a dynamic programming method as a subroutine to compute the optimal crawl plan for single sensors. 
A fast greedy method is also proposed as an complementary.
(3) We evaluate EasiCrawl with simulation and real world data crawled from the IoT platform Xively\cite{xively}.

The rest of the paper is organized as follows. 
We formalize the optimization problem in section \ref{formulation}, then introduce EasiCrawl in section \ref{easicrawl}. 
Simulations are performed in section \ref{simulation} and a case study is introduced in section \ref{case_study}. 

\section{Related Work}\label{related_work}

There are a lot of off-the-shelf technologies that can be used for IoT search systems.
We have investigated these network configure protocols used in LAN to automate discover domestic devices, like UPnP\cite{UPnP} and Bonjour\cite{Bonjour}.
Although these device discovery mechanisms are widely used, they are actually not suitable for IoT applications, mainly because of high power consuming. 
These auto-configuration protocols are designed for devices with sustained power supply, while IoT sensors usually face energy limitation challenges.
More densed searches for sensors in the sleep mode are not affordable.
In addition, the device description language used by these protocols are relatively weak, which can not define the realtime state of IoT sensors. 
As a result, these protocols can not be widely used in IoT search system.

Besides the off-the-shelf technologies, a lot of new research of IoT search are conducted recently. 
Many IoT search systems with different architectures have been proposed.
According to how the sensor information is gathered, these architectures can mainly be summarized into two categories, namely push-based and pull-based search systems respectively.
In a push-based system, the integrity of data is in prior.
Sensors periodically send their data and servers store the data once received. 
Search requests of these push-based search systems are running on continuous datastreams or perform query directly in databases using SQL. 
Technologies like TinyDB\cite{TinyDB} and IoT feeds\cite{Whitehouse2006} take IoT sensors as datasources, and process structured command to query the data stream generated by these sensors. 
This kind of IoT systems are easy to be implemented, but the redundancy of raw sensor data may causes problems in energy and data transitions.
In many applications, not all data is need to be sent and uploaded to server. 
This problem seems inevitable because sensors don't know exactly which piece of information needs to be upload.
Alternatively, pull-based search systems may be the solution to this problem.
Pull-based architecture use semantic description, i.e. XML, to describe IoT sensors. 
Sensors update their description based on data collected, making their behavior very similar to that of a web page.
This property makes searching IoT with a traditional web search engine possible. 
For example, SPITFIRE\cite{SPITFIRE} uses crawlers and modern indexing methods, along with semantic IoT technology as IoT sensor descriptions. 
Previous works of semantic IoT\cite{Compton2012} and the propose of CoRE\cite{CoREWorkingGroup2012} are devoted to standardizing IoT sensor description languages and building a web-style IoT search system. 
The description files can be accessed by simply retrieving the ./well-know files used in the CoRE framework. 
Although the communication over-head of pull-based architecture is relatively heavy, the scalability make pull-based IoT search architecture very promising.
EasiCrawl are implemented on a pull-based IoT search architecture similar to Spitfire.

Crawlers Scheduling is a well studied topic in the field of web search. 
Previous works\cite{Cho2000}\cite{Wolf2002}\cite{Challenger2004} analyzed how the crawler strategy affects the freshness of the repository of web pages maintained by web crawlers, which determines the query hit rate and whether this system can return a up-to-date result. 
But in an IoT search system, the sleep behavior of sensors may significantly impact the latency of the crawl of information.
Also, to the best of our knowledge, no research has yet been done to study the crawl schedule with energy constraints.

\section{Formulation}\label{formulation}

In this section, we formulate the crawl schedule problem, and introduce all the symbols used in the following paper. The assumptions followed are set to simplify sensors' working model.
Firstly, events that users care arrive at each IoT sensor in a Poisson manner, where the time interval between any two events follows exponential distribution. 
Secondly, when sensors are in sleep mode, they can not be accessed by crawlers, and no new events can be captured or stored.
Thirdly, sensors' sleep plan is previously known.
Last, the length of work or sleep cycle does not need to be equal.

EasiCrawl is invoked periodically during time period of $T$.
$T$ is called the \textbf{time range} of EasiCrawl in the following paper.
We assume there are totally $N$ sensors that need to be crawled.
A discretization method is applied for model simplicity, and finite numbers of time points can be used for crawl.
We set up a discrete time table with a \textbf{discretization step length} $\epsilon$, each crawl must be preformed at one of these time point in the time table.
Our goal is to find a property \textbf{schedule plan} $\Phi$ so that the expected latency of all $N$ IoT sensors is minimized. 
The schedule plan consists of two part. 
The first part is the \textbf{arrangement of crawls} $\hat{n}$ for each sensor $i$, denoted as $\hat{n}=n_1,n_2,\ldots,n_{N}$. 
We also use the symbol $\Phi(\hat{n})$ to indicate the schedule plan with arrangement $\hat{n}$.
The second part is the specific \textbf{crawl plan} for each sensors, which is a list of time points for crawling. 
We denote this list as $\phi_{i}$ for convenience.
In order to model the sleep pattern of each sensor, we use a list of time slots to represent the working duty cycle of sensors, which is called \textbf{sleep plan} for brief. 
The discretization process takes the sleep plan of each sensor as input, and generate discretized time points for each sensor, where time points of sleeping period of sensors are not included.
Sometimes the importance of sensors are different, so we define a factor $\omega_i$ as the expectation weight of sensor $i$. 
Here we use the notation $E_{L}(n_{i})$ as the minimal \textbf{expected latency} that can be gained by sensor $i$ with totally $n_i$ chances to crawl.
The final object function $E_L(\Phi(\hat{n}))$ is the sum of expected latency of all sensors which is denoted as $\sum_{i=1}^{N} \omega_i E_{L}(n_{i})$.

In addition, this problem follows two constraints. 
As previously described, the crawl server has constrained server load and IoT sensors' energy budgeted for sensors in IoT are limited. 
Hence, for the server-side, we set a constraint that the crawlers from the server can at most commit $\theta$ crawls during the time range $T$. 
During the time range $T$, the access times $n_i$ for sensor $i$ is limited to be less than $\gamma_i$ times. 
With the object function and constraints analyzed above, we get the following formula.
The final output of EasiCrawl is crawl plan $\Phi$, which contains the optimal crawl arrangement $\hat{n}$ and crawl plan $\phi_i$ for each sensor $i$.
\begin{eqnarray}
\begin{array}{ll}
\min_{\Phi}& E_L(\Phi(\hat{n}))\\
\text{s.t.}
& \sum_{i=1}^{N} n_i \leq\theta\\
&n_i\leq\gamma_i\\
&i=1,\ldots,N
\end{array}\label{OBJ}
\end{eqnarray}

The generalized formulation \eqref{OBJ} does not require specific sensor model or sleep plan. 
Different sensor models can be used for this formulation, as long as the relationship between $n_i$ and $E_L(n_i)$ can be determined.
Later we will show that, this problem can be solved in polynomial time, given the fact $E_L(n_i)$ of each sensor are convex functions.

\section{Crawling IoT Sensors with EasiCrawl} \label{easicrawl}

We first give out method framework of EasiCrawl, which iteratively improves our solution to achieve a latency optimal scheduling plan. 
Then technical details of subroutines of EasiCrawl is introduced, including the schedule method for periodic or none-periodic sleep sensors. At last, we discuss the strategy for sensors whose sleep plan is previously unknown.

\subsection{Method Framework}
The main idea of EasiCrawl is to change crawl arrangements $\hat{n}$ according to the gradient of $E_L(\Phi(\hat{n}))$.
This arrangement is done in an iterative manner.
Firstly, an original arrangement is made in proportion to an estimated latency, which is the value of sensors' importance $\omega$ multiple length of working time.
Then, EasiCrawl starts to improve the object function $E_L(\Phi(\hat{n}))$ step by step. 
In each iteration, EasiCrawl searches for a best change in arrangements which can deduce the expected latency most quickly.
The step length for change is denoted as $\epsilon$, which is $1$ by default.
With these analysis, the minimization problem can be transformed into the following two sub-problems.
First, how to rearranging crawl numbers for each sensor under two constraints? 
Second, what's the best crawling plan for each sensor? 
Once the count of crawls $n_i$ is chosen, the optimal crawl strategy for a single sensor is determined. 
To iteratively improve the global result, we use an approach to change a pair of $n$ at the same time.
We start to search sensors for a pair $i$, $j$, where $i$, $j$ are both arrangements. 
If $i\gets i-1$ and $j\gets j+1$ will cause a largest improvement to the object function compared to the other pairs, then changes of $i$ and $j$ are conformed. 
This process will be continued until none improvement can be made. 

\begin{algorithm}
	\caption{EasiCrawl Method Framework}
	\label{alg:framework}
	\begin{algorithmic}[1]
		\renewcommand{\algorithmicrequire}{\textbf{Input:}}
		\renewcommand{\algorithmicensure}{\textbf{Output:}}
		\REQUIRE $\mathbf{T}$, $N$, $\theta$, $\epsilon$
		\ENSURE  $\phi$
		\\ 
		\STATE $\hat{n}\gets EvenlyDivide(\theta), LastExpect\gets 0$
		\WHILE {$E_L(\Phi(\hat{n}))-LastExpect>\epsilon$}
		\STATE $BestChange \gets 0, i'\gets 0, j'\gets 0$		
		\FOR {each sensor pair $i,j\in$ sensors}
		\STATE $\hat{n}'\gets n_1,...,n_i+1,...,n_j-1,...,n_N$
		\STATE $c\gets E_L(\Phi(\hat{n})) - E_L(\Phi(\hat{n'}))$
		\IF {$c > BestChange$}
		\STATE $BestChange \gets c, i'\gets i, j'\gets j$
		\ENDIF
		\ENDFOR
		\STATE $\hat{n}\gets n_1,...,n_{i'}+1,...,n_{j'}-1,...,n_N$
		\ENDWHILE 
		\RETURN $\Phi$
	\end{algorithmic} 
\end{algorithm}

The above pseudo code \ref{alg:framework} explains the workflow of EasiCrawl. 
Some input parameters are omitted for brief, namely weights of each sensor and discretization step length.
In step 1, method $EvenlyDivide$ initialize the crawl arrangements $\hat{n}$. 
Schedule plan $\Phi$ is iteratively improved during steps 2 to 11. 
EasiCrawl search for the best change from Step 3 to 10, and then replaced $\Phi$ with a better one.

Here, we explain why this method leads to optimal schedule.
Indeed, the correctness of EasiCrawl bases on the fact that the expected latency function for each sensor $E_L(n_i)$ are convex. 
If $E_L(n_i)$ can be proved convex, the convexity of the object function $E_L(\Phi(\hat{n}))$, which is the sum of $E_L(n_i)$, can be proved.
Notice that the constrains are convex, \textbf{problem \eqref{OBJ} is indeed a convex optimization problem}, in which iterative method always points to a optimal solution.
Next, we can prove that $E_L(n_i)$ is convex for sensors with sleep behavior.
More specification of sensor's sleep plan is needed for this proof.
First, we use $E_f(t_s,t)$ to represent the expected latency of a time period, where $t_s$ and $t$ are the start time and end time. 
If the crawl plan are located at $t_1,t_2,...,t_n$, then expected latency of a single sensor can be rewrote as $E_L(n) = \sum_{i=1}^{n-1}E_f(t_{i},t_{i+1})$. 
We use $E_h[G(t)]$ as an alias for $E_f(t_s,t)$ for simplify.
The convex of expected latency $E_h[G(t)]$ is proved in the appendix. 
Notice that for any optimal crawl $t$ in the middle of two adjacent time interval $[t_a,t]$ and $[t,t_b]$, $E_f(t_a,t+\varepsilon)+E_f(t-\varepsilon,t_b) = E_f(t_a,t-\varepsilon)+E_f(t+\varepsilon,t_b)$ holds for any infinitesimal $\varepsilon$, otherwise $t$ must be at the end of one work cycle.
For the former case, the expectation function $E_h[G(t)]$ of time periods $[t_i,t_i+1]$ can be consider the same, while the latter one hold a expectation bound less than the former.
The convexity of $E_L(n) = n E_h[G(t/n)]$ can be proofed by the convex preserve operations\cite{boyd2004convex}. 
In this way the convexity of $E_L(n)$ can be proved.
Our result in simulations also shows a supportive result for these analysis.
In addition, the computation of $E_h[G(t)]$ and $E_f(t_s,t)$, which are both expected latency of a single senor, is discussed in the appendix.

In the following paper, we start to design subroutines for the computation of $E_L(n_i)$.
At first, A dynamic programming method is put forward for an optimal solution,  but the time complexity is somewhat unacceptable for large datasets.
So, we proposed a greedy method as a trade-off for time efficiency. 
All these two methods are used to compute the crawl plan $\phi$ of a single sensor with sleep behavior, which leads to a optimal expected latency $E_L(n_i)$.

\subsection{Dynamic Programming Method for Optimal Crawl}

In a real world IoT system, sensors' sleep plan make them difficult to crawl in a fix interval. 
Actually, the best crawl plan may be very different from a periodic crawl, especially when the sleep plan is not periodic.
With the previous discretization process, we have a chance to enumerate all the possible crawl plans. 
The following dynamic programming(DP) method take advantage of memorized search, which caches the solution of sub-problem to accelerate the enumeration process.

We use \textbf{timeline} to indicate the discretized time table for a single sensor, which consists of the working cycle and the sleeping cycle.
The crawl plan can actually be formed as a route planning problem given the following transformation.
Here a directed acyclic graph(DAG) model $G(E,V)$ is used to formalize this problem.
$E$ are the edges of the graph, and $V$ are the vertex. 
Every points standing for the working cycle as graph vertex is written as $v_i$. 
Suppose there are $m$ vertexes and $T$ time points in the timeline. 
Corresponding, there are exactly $T$ vertexes in this graph, from number $1$ to $T$. 
We define $n$ as the count of crawls used, which indicates there are $n-1$ vertex allowed to pass, because the last vertex must be chosen. 
For each pair of time points $t_i$ and $t_j$, if $t_i<t_j$, a directed edge $e_{i,j}$ from $v_i$ to $v_j$, is added to the graph. 
A weight value $w_{ij}$ is set to $e_{i,j}$, whose value is $E_f(t_j, t_i)$.
$E_f(t_j, t_i)$ is actually the expected latency gain of a crawl at time $t_j$ whose previous crawl is at $t_i$.
The problem of minimized-latency for a single sensor is now transformed into that, how to find a path $e_{1,i},\ldots,e_{j,T}$ from $v_1$ to $v_m$, which take exactly $n$ steps, that minimum the sum of all the weights alongside. 
This transform process can be illustrated by Fig.\ref{fig:problemtrans}.
\begin{figure}
	\centering
	\includestandalone[width=\linewidth]{pic/problemtrans}
	\captionsetup{justification=centering}
	\caption{Example of Non-periodic Hibernate Problem Transformation}
	\vspace{-1.5em}
	\label{fig:problemtrans}
\end{figure}

With an acceptable scale of time range $T$ and crawl time $n$, this problem can be solved by DP shown in Algorithm \ref{alg:dp_min}. 
Solution state with a vertex $v_i$ and the available crawl time $n'$ left is an sub-problem. 
The optimal expectation can be computed by adding weight $w_{i,j}$ with all the expectation known in sub-problems. 
The recursive relationship can be summarized with expression \eqref{dp}.
\[f(v_i, n')=\left\{
\begin{array}{ll}
\min_{k=1}^{i-1} & \{f(v_{i-k}, n'-1)+w_{i,i-k}\}\\
& \text{if } i\in[1, m-1]\\
0 & \text{if } n'=0 \text{ and } i=1\\
\infty & \text{otherwise}
\end{array}\numberthis \label{dp}
\right.
\]

$f(v_i,n')$ is the sum of weight of a path that start from $v_1$ and end with $v_i$, with cost exactly $n$ steps. In each step, we go over the timeline for $T^2$ times.
So the time complexity of iterating process is $O(m^2n)$, in which $nm$ sub-problem are computed and each computation will cost $m$ operations. 
The backtrack method uses $p(v_i, n)$ to get the optimal strategy, which takes at most $n$ steps. 
As a result, the overall time complexity is still $O(m^2n)$.
\begin{algorithm}
\caption{DP Method for Optimal Crawl}
\label{alg:dp_min}
\begin{algorithmic}[1]
	\renewcommand{\algorithmicrequire}{\textbf{Input:}}
	\renewcommand{\algorithmicensure}{\textbf{Output:}}
	\REQUIRE $G(V,E)$, $n$
	\ENSURE  $t_1,\ldots,t_n$
	\\
	\STATE $f(v_i,n') \gets \infty$, $p(v_i,n')\gets 0$
	\FOR{$n'=1$ to $n-1$}
	\FOR{$i=2$ to $m$}
	\STATE $f(v_i,n')\gets\min_{k=1}^{i-1}\{f(v_{i-k}, n'-1)+w_{i,i-k}\}$
	\STATE $p(v_i,n')\gets$ minimum $v_{i-k}$
	\ENDFOR
	\ENDFOR
	\STATE $t_n\gets T$, $t_i\gets$back trace from $p(v_i, n)$ for $t_i$
	\RETURN $t_1,\ldots,t_n$
\end{algorithmic}
\end{algorithm}

\subsection{Greedy Method for Time efficiency}

Previous DP method is a generic way to compute the accuracy crawl plan. 
Although it can return a optimal solution, the computation complexity is relatively high.
Most of the time, there is no need to compute a optimal plan, especially when the computation resource is intensive.
With more insights, we find crawl planning of a single sensor can be boosted in some special case.
In this section, we introduce a greedy crawl planning method, which is a perfect approximation.

We start from the inner property of crawl planning for a single sensor. 
This problem actually can be formalized by equation \ref{greedy_obj}. 
Here $E_f(t_i,t_{i+1})$ are the expectation function for a time period.
\begin{eqnarray}
\begin{array}{ll}
\min_{\textbf{t}}& \sum_{i=1}^{n} E_f(t_i,t_i+1)\\
\text{s.t.}
& t_i < t_{i+1}\\
& i=1,\ldots,n-1
\end{array}\label{greedy_obj}
\end{eqnarray}

The result of this problem is actually $E_L(n)$, the minimal latency that can be achieved with $n$ crawls.
If both $E_f(c, t_i)$ and $E_f(t_i,c)$ are convex, where $c$ is a constant, then this problem is a convex optimization problem, which can be solved using greedy method.
In the follow analysis, we looked into two special situation, and explained why greedy method can not achieve the optimal solution in the most general case.

In the first case, we assume the sensor works continuously.
An intuition to solve this problem is to evenly distributed the crawling action across the timeline. 
We prove this method actually leads to an optimal value.
\begin{lemma}
\label{evenly}
The optimal crawl plan of an continuously working sensor is to scatter the crawl action over the timeline $t\in [0, T]$ as evenly as possible, where $T$ is a constant.
\end{lemma}
\begin{IEEEproof}
This proposition can be proved by the convexity of $E_h[G(t)]$ \cite{boyd2004convex} with induction. 
Here we donate $E_h[G(t)]$ as $f(t)$ for convenience.
We start from the situation where only two crawling are allowed. 
As a result, the expected latency gain during $[0,T]$ is marked as $F(0, T)$.
$F(0,T)$ is divided into $f(t)$ and $f(T-t)$ with a time point $t\in[0,T]$. 
With the convexity of $f(t)$, we have $2f(T/2)<f(T/2-\epsilon)+f(T/2+\epsilon)$ for $\forall{\epsilon>0}$. 
By plugging in $F(T)$, we get $F(T/2)<F(T/2+\epsilon)$ for $\forall{\epsilon>0}$.
This indicates $T/2$ is the minimum point of $F(t)$, which means the evenly crawl is the best strategy for the situation with only $2$ crawls.
Now suppose there are $n$ crawling actions, among which first $n-1$ actions evenly divide the timeline.
For last two pieces of crawl intervals divided by the $n$-th crawl, we can choose the evenly one as a new strategy, which leads to a larger $f(t)$.
\end{IEEEproof}

With Lemma \ref{evenly}, we can get the minimal gain of latency for different crawl time $n$.
If the sensor works without any sleep, the relationship between crawl time and $E[\phi(n)]$ can be computed with $E_h[G(n)]=nE[G(\frac{T}{n})]$.
According to the result from appendix, $E_h[G(n)]$ has an close-form expression as $T(T\lambda/n-1+e^{-T\lambda/n})$. 
With this function, the minimum expectation can be calculated directly.

Then we illustrated that this intuition holds for the situation when sensor work periodically, with some constraints.
Some points in the timeline can not be accessed, the expectation function in this situation is actually piecewise.
Here we use $T^{w}$ and $T^{s}$ as the length of working and sleep cycle respectively.
If sensor works periodically, the intact cycle is marked as $T^{c}$, where $T^{c}=T^{w}+T^{s}$.
The intuition to solve problem is to arrange the crawls at the end of a work cycle.
We show this intuition is indeed an optimal strategy when the crawl interval is an integral multiple of duty cycle $T^{c}$.
\begin{lemma}
\label{evenly_s}
For periodically sleeping sensors, suppose the sensor's duty cycles has exactly an integral multiple of numbers of crawls, indexed from $1$ to $m$.
The optimal strategy is to make $n$ times of crawl at the end of a working cycle, whose index $i$ satisfies $i \bmod (m/n)=0$.
\end{lemma}
\begin{IEEEproof}
We still use induction for this proof.
For the most simple case, when number of crawls $n=2$, the first crawl locates at time $t'$ and second one at the end. 
This strategy is optimal due to any increase of $t'$ will cause a gain in the object function by $E_f(0,t'+T^{w})-E_f(0,t')$, and any decrease of $t'$ will raise the object function due to the evenly divide.
For the case when $n>2$, the problem can be constructed by add $t'/T^{c}$ cycles, where if Lemma \ref{evenly_s} holds for $n$, it must holds for $n+1$.
\end{IEEEproof}

Based on previous analysis, we put forward a greedy method, which tries to divide the crawls as evenly as possible. 
The result can be computed in one pass, so the complexity of this approach is $O(n)$, where $n$ is the size of crawl times.
\begin{algorithm}
	\caption{Fast Greedy Crawl Method}
	\begin{algorithmic}[1]
		\renewcommand{\algorithmicrequire}{\textbf{Input:}}
		\renewcommand{\algorithmicensure}{\textbf{Output:}}
		\REQUIRE $n$, $T$, $T^w$, $T^s$
		\ENSURE  $t_1,\ldots,t_n$
		\\ 
		\STATE $T^{c} \gets T^{w}+T^{s}$
		\STATE $m \gets T/(T^{c} n)$, $r\gets (T/T^{c})\bmod{n}$
		\FOR{$i=1$ to $m$}
		\STATE $t_{m-i+1}\gets T- T^{c} i + T^{w}$
		\IF{$i\ne 0 \land r\ne 0$}
		\STATE $t_{m-i-1}\gets t_{m-i+1}-T^{c}$
		\STATE $r\gets r-1$
		\ENDIF
		\ENDFOR
		\RETURN $t_1,\ldots,t_n$
	\end{algorithmic} 
\end{algorithm}

At last, we discuss the most general situation. 
When the length of working cycles are not the same, or the number of duty cycle is not divisible by $m$, or the length of duty cycles are different, the previous intuitions no longer holds.
In these situations, the greedy method for searching crawl plan can not lead to a optimal value. 
It's because the expectation function with two variables $E_f(t_s,t)$ is not convex when $t$ is fixed.
As a consequence, local minimum may exists in the result, which makes the achieved by greedy method possibly not the optimal one.
Even though, we add this non-optimal method in our final implementation, because of its good performance in real world datesets.

\section{Simulation and Evaluation} \label{simulation}

In this section, we will test the performance of EasiCrawl under different settings and input parameters. 
\subsection{Settings}
Expected latency which are previously used as a optimization objective here is taken as a metric of measurement.
Performance of EasiCrawl is measured with different settings of following parameters, including sensor scales $N$, total crawl limit $\theta$, discretization step length $\epsilon$, and time range $T$ of scheduling. 
We use a randomized method to generate the sleep plan for each sensor, and we used a Poisson process generator to generate the simulation events.
For comparison, we implement a random schedule method, called RandomCrawl. 
RandomCrawl use the same method as EasiCrawl to initialize the arrangement of crawls to different sensors. 
Then, for each sensor, the crawl plan are set randomly, the last timestamp are chosen to ensure all the events are captured. 
All these simulation are implemented on a PC with MATLAB.
\begin{figure*}
	\minipage{0.32\textwidth}
		\includestandalone[width=\linewidth]{pic/test1_sensorscale}
		\captionsetup{justification=centering}
		\caption{Expected latency With Different Sensor Scales}
		\label{fig:test1_sensorscale}
	\endminipage\hfill
	\minipage{0.32\textwidth}
		\includestandalone[width=\linewidth]{pic/test2_totalcrawl}
		\captionsetup{justification=centering}
		\caption{Expected latency With Different Crawls}
		\label{fig:test2_totalcrawl}
	\endminipage\hfill
	\minipage{0.32\textwidth}%
		\includestandalone[width=\linewidth]{pic/test3_discretestep}
		\captionsetup{justification=centering}
		\caption{Expected latency With \\Different Discretization Step}
		\label{fig:test3_discretestep}
	\endminipage
	
	\minipage{0.32\textwidth}
		\includestandalone[width=\linewidth]{pic/test4_timerange}
		\captionsetup{justification=centering}
		\caption{Expected latency With \\Different Total Time Range}
		\label{fig:test4_timerange}
	\endminipage\hfill
	\minipage{0.32\textwidth}
		\includestandalone[width=\linewidth]{pic/test5_sensortype}
		\captionsetup{justification=centering}
		\caption{Expected latency With \\Different Plan Methods}
		\label{fig:test5_sensortype}
	\endminipage\hfill
	\minipage{0.32\textwidth}%
		\includestandalone[width=\linewidth]{pic/test6_convergespeed}
		\captionsetup{justification=centering}
		\caption{Converge Speed under \\Different Crawl Times}
		\label{fig:test6_convergespeed}
	\endminipage	
	\vspace{-1.0em}
\end{figure*}

\subsection{Results}

We first study the impact of \textbf{sensor scale $N$}. 
We mainly test the latency of dynamic programming schedule method of single sensor in this simulation. 
All the sleep plan of sensors are previously known in advance.
The total crawl limit $\theta$ is set as the multiple of sensor scale, varies from 10 to 200. 
The Poisson parameter $\lambda$ of each sensor is randomly chosen from 0 to 1. 
The schedule time range $T$ is set as 200 time units. 
In each improvement iteration, the crawl time arrangement $n_i$ for different sensors are changed by the step length of 1. 
For each sensor scale, we repeated the simulation for 5 times, and compute the average, minimum and maximum value of expected latency. 
Simulation result is plot in Fig. \ref{fig:test1_sensorscale}.
In the result, EasiCrawl surpass the RandomCrawl by 2 times in reducing the expected latency, even take error bar into consideration. 
This is mainly because of the theoretical optimal property of EasiCrawl when given a sequence of discretized time nodes. 

\textbf{The total number of crawls $\theta$} reveals the limit of the IoT search engine.
In this test, EasiCrawl worked on sensor scale of 10, crawling time range of 200, and discrete step length of 2, where minute is the unit. 
We use RandomCrawl as a reference, and test 40 different crawl limits on EasiCrawl and RandomCrawl, ranging from 10 to 50, which show a result as Fig.\ref{fig:test2_totalcrawl}.
The result first show the advantages of EasiCrawl against RandomCrawl. 
More important, we notice there's a submodular relationship between total crawls and the latency reduction, which means the gain of server side crawl ability pay less when limits grow larger. 
This is important for us to find a proper trade-off between server cost and schedule efficiency.

Time table of sleep plans should be discretized with a \textbf{discretization step length $\epsilon$} before starting scheduling. 
The choice of discretization step length is a trade off between the computation time and final accuracy. 
We use a simulation with 10 sensors and a time range $T$ of 500, to test the relationship between time complexity and expectation accuracy. 
The total crawl count $\theta$ is set to 100 times.
We carried on 20 groups of tests, where the discretization step length $\epsilon$ ranges from 1 to 20.
Fig.\ref{fig:test3_discretestep} shows a positive result to our analysis of the dynamic programming crawl method used in EasiCrawl. 
While expected latency increase about 400 units, the computation time decreased from 1 seconds to below 0.1 seconds. 
This result fits the theoretical analysis, where for scheduling each sensor, the time complexity is $O(m^{2}n)$, $n$ is the crawl limit and $m$ is the number of time points.
When $\epsilon$ is relatively small, the number $m$ of total time nodes which can used to crawl is large, leading to a higher computation cost.

\textbf{Time range $T$} directly affects the executing time and running frequency of EasiCrawl.
We use the same experiment settings as that of discretization test. 
10 simulated sensors are used, the total crawl sum $\theta$ is 30. 
For time range $T$ varies from 50 to 1000, a new time table is generated in each iteration.
$T$ has a similar effect as the discretization step $\epsilon$. 
The computation time showed in Fig.\ref{fig:test4_timerange} is very close to the analysis result, which quadratic relationship between $T$ and running time can be inferred. 
The expectation shares a similar shape because the distance between the distribution and the end time $t$ can also be approximated as a quadratic function.

Two totally different \textbf{plan methods}, the DP method and the greedy one, are used in EasiCrawl.
In the previous evaluations, only the DP methods are used for finding the best schedule for each sensors. 
In this test, both of the methods are tested. 
Tests are performed for 10 times with different sensor scale varies from 10 to 100. 
4 different group of sensors are used for each test. 
Sensors in the first group are all scheduled with DP method.
In the second group, half of the sensors are, while the other half use greedy based method. 
The third group used pure greedy based method. We also use the RandomCrawl method as a reference, which is the fourth group.
Result are showed in Fig.\ref{fig:test5_sensortype}. 
As our expect, DP method performs the best in latency reduction. 
With the share of greedy method grows, the expected latency becomes larger. 
When all the sensors use greedy method for scheduling, the expected latency is the worst, but still much better than that of RandomCrawl.

The \textbf{speed of convergence} directly affects the efficiency of the total computation cost. 
Here we set up a test with two different total crawl times $\theta$, to verify the iteratively optimization method. 
The time range $T$ here is chose to be 400 time unit. 
We separately perform this test, first use a total crawl limit $\theta$ of 50, then 100. 
The sleep plan of sensors and other parameters are the same. 
The total iterations used before reaching convergence is collected.
Fig.\ref{fig:test6_convergespeed} is the final statistics, where the iteration steps for different experiment setting is close to Gaussian distribution. 
Apparently, when the number of total crawls grow bigger, a larger mean value is expected.

\section{Case Study With Xively} \label{case_study}

Xively is a platform which integrates IoT sensors. 
With thousands of sensors that can be simply accessed with through ip protocols, Xively become a ideal platform for our crawl method.
We built a mini IoT search system to test our crawl scheduling method EasiCrawl, where we continuously monitored several datastream of 3 airqualityegg\cite{airegg} sensors located in London and Tokyo as an example for our case study. 
Actually, accessing the sensors is accomplished by query the Xively database which hold the up-to-date sensor information. 
This makes no different for us than directly accessing a real sensor. 
One problem here is that most raw sensors on Xively don't support semantic senor description functions. 
So we add a describing layer, which translate raw data streams to event sequence, by generating a log for the abnormal events in the raw data. 
For example, a sudden raise in temperature or violation of radiation limit are taken as events, which will be stored as a item of description.
We consider the high level of CO, NO\textsubscript{2}, are danger events, and the quick raise in temperature or quick changes in humidity may be caused by the miss use of electrical equipments, which should be also recorded. 
We do not consider the temperature and humidity changes in the day time as interesting events, only to test the effectiveness of scheduling non-periodic sensors.

\begin{figure}
	\centering
	\hspace{-1.0em}
	\includestandalone[width=\linewidth]{pic/xivelycasestudy}
	\captionsetup{justification=centering}
	\caption{Crawl Plan Generated by EasiCrawl for \\
		Data from Xively}
	\vspace{-1.5em}
	\label{fig:xivelycasestudy}
\end{figure}

In this case, the parameter for Poisson process of the arriving event sequence are estimated periodically before EasiCrawl is invoked.
Fig.\ref{fig:xivelycasestudy} is the raw sensor data of 4 different data streams, including CO, Humidity, NO\textsubscript{2}, along with the corresponding scheduling result. 
The circle marks are the event captured and recorded which users may care. 
The triangle marks are the actual crawls. 
EasiCrawl can always hold a low latency level, which is 2 to 3 times lower than that of RandomCrawl.
In this particular case, the average latency of the RandomCrawl is about 23 hours, while for EasiCrawl, the time is reduced to 13 hours. 

\section{Conclusion} \label{conclusion}

In this paper, we introduce EasiCrawl, a crawl scheduling strategy which runs an iteratively improving method. 
EasiCrawl can be used with sensors which sleep periodically, which is ideal to be used in IoT search system. 
Effectiveness of EasiCrawl is also showed in simulations and a case study with Xively. 
The drawbacks of EasiCrawl lies in the blindness when crawling sensors whose sleep plan is unknown. 
So, automatic parameter choosing mechanism is urgently needed.
Besides further works should be done to enhance the description layer in our prototype.

\section*{Acknowledgment}

This paper is supported by the International S\&T Cooperation Program of China (ISTCP) under Grant No. 2013DFA10690, the ``Strategic Priority Research Program" of the Chinese Academy of Sciences under Grant No. XDA06010403, and the National Natural Science Foundation of China (NSFC) under Grant No.61100180.

\ifCLASSOPTIONcaptionsoff
  \newpage
\fi


\bibliographystyle{unsrt}%{IEEEtran}%{unsrt}
\bibliography{reference}


\appendices
\section{Analysis of Expected Latency}

\subsection{Expectation of Sensor with Sleep Behavior}

In this section, we first introduce a simple way for deducing $E_f(t_s, t)$ when sensor works without sleep.
Then we discuss the computation of expected latency $E_L[G(t)]$ where sensor has sleep behavior.
At last we prove $E_L[G(t)]$ is a convex function of $t$.

We suppose the IoT sensor crawled start working at time $0$ for convenience. 
The total expected latency when crawl at time $t$ can be considered as an random process $G(t)$. Our goal can be rewritten to compute the expectation $G(t)$, written as $E[G(t)]$. 
Notice $E[G(t)]$ can be divide into different parts due to the count $N(t)$ of events which happen during $[0, t)$, then $E[G(t)]$ can be written as:
\begin{equation}
E[G(t)] = \sum_{n=1}^{\infty} E[G(t)|N(t)=n]P(N(t)=n) \label{EG}
\end{equation}

$P(N(t)=n)$ is the probability of having $n$ events during $[0,t)$, which follows Poisson distribution, where $P(N(t)=n)=e^{-\lambda t}{(\lambda t)^n}/{n!}$ holds. 
$E[G(t)|N(t)=n]$ can be resolved as $\frac{n}{\lambda}(t\lambda+e^{-\lambda t_m}-1)$. 
$t_{m}$ stands for the happened time of the $m$-th event. 
By taking $E[G(t)|N(t)=n]$ into \eqref{EG}, we get the close form solution of the expectation as equation \eqref{ExpectCont}.
\begin{equation}
E[G(t)] = (e^{-t\lambda}-1+t \lambda)t \numberthis \label{ExpectCont}
\end{equation}

For sensors with sleep behavior, we suppose sensor starts to work at time $t_s$, and begin to work with a sleep plan.
Here we mark $T^{c}_{i}$ as the $i$-th working cycle. 
$T^{c}_{i} = T^{w}_{i}+T^{s}_{i}$, where $T^{w}_{i}$ is the $i$-th working cycle and $T^{w}$ is the $i$-th sleeping cycle. 
It's hard to directly deduce an expectation for a Poisson process whose timeline is break up.
However, with equation \eqref{ExpectCont}, we can compute the $E_f(t, t_s)$ by subtract the expectation cause by the previous crawls from $E[G(t)]$. 
In this recursive way, the expectation can be calculated. 
Here $E_h[G(t)]$ stands for the expectation to crawl a sleep enabled sensor at time $t$, $E[G(t)]$ still stands for the expectation if this particular sensor doesn't sleep.
Start time $t_s=0$, $E_f(t_s,t)$ can be written as $E_h[G(t)]$.
Suppose time $t$ locates in an working cycle, written as $t \in [t_s + \sum_{i=1}^{k} T^{c}_{i}, t_s + \sum_{i=1}^{k} T^{c}_{i} + T^{w}_{i}]$, where $k\in\mathbb{N}$. 
This assumption is based on the fact that, any crawl at a sleeping cycle can be bring forward to last work cycle for a better expected latency.
\begin{align*}
E_h[G(t)] & = E[G(t-T^{L}(K))] + \\
	& \sum_{i=1}^{K}(E[G(t-T^{L}(K-i))]-\\
	& E[G(t-T^{L}(K-i)-T^{w}_{K-i})])\\
	& = (t-T^{L}(K))^2\lambda + \sum_{i=1}^{K} (2 t - T^{w}_{K-i})T^{w}_{K-i}\lambda 
	\numberthis \label{ExpectSleep}
\end{align*}

Here $T^{L}(K) = \sum_{j=1}^{K} T^{c}_{j}$, which is time duration of first $K$ cycles, and $K$ is the count of cycle $T^{C}$. 
For any time $t$ in a sensor working cycle, by summing those expectations before time $t$, the overall expectation of latency can be achieved when take sensor sleep cycle into consideration. 
The first factor of \eqref{ExpectSleep} is the expectation of last working cycle, and the sum is the expectation of the previous working cycle.

Expression \eqref{ExpectSleep} is a simplified form of $E_h[G(t)]$.
We use this simplified version to show $E_h[G(t)]$ is indeed convex.
Here we try to compute the one-order derivative on $t$.
\[\frac{\partial E_h[G(t)]}{\partial k} = \left\{
\begin{array}{lr}
c_i+\gamma_i t & \text{if }t \in [T^{L}_{i}, T^{L}_{i}+T^{w}]\\
c_i & \text{if }t \in [T^{L}_{i}+T^{w}, T^{L}_{i+1}]
\end{array}	\numberthis \label{ExpectSleepAnother}
\right.
\]

The result is a continuous linear piecewise function, where $c_i$ and $\gamma_i$ are constraints.
The meaning of expression \eqref{ExpectSleepAnother} is actually the gain speed of expected latency. 
With expression \eqref{ExpectSleepAnother}, the second order derivation $\nabla^{2} E_h[G(t)]\geq0$ in its domain, and the convexity of function \eqref{ExpectSleep} is proved. 

\subsection{Expectation of Arbitrary Period}
Previous analysis computes the expectation with the start time at $0$. 
The problem of computing $E_f(t,t_s)$ if $t\ne0$, can be solved given the previous analysis. 
This problem can be divided into two sub-problem by value of $t_s$.
Assume that $t_s\in [0,T^{c}_{1}]$, other case can be transformed to this by shifting $t_s$ and $t$.
If $t_s$ is in a sleeping cycle, then expectation can just be calculated from the next work cycle.
Otherwise, if $t_s$ is in a working cycle, the expectation can be estimate by $E_h[G(t)]$ which start at $0$ subtract the expected latency cause by the events arriving at time $[0, t_s]$.

We define $t'=t-(t_s\bmod T^{c}_{1})$ and $t''=t-T^{c}_{1}$ for convenience, where $T^{c}_{1}$ is the length of first cycle.
Here the minuend means the expectation at time point $t$ of latency during time period $[0, t_s]$.
If $t_s$ is at a hibernate cycle, then the result expectation is just as the one which $t_s$ equals next duty cycle, which is $E_h[G(t')]$, which can be written as follows.
\[E_f(t,t_s)=\left\{
\begin{array}{lr}
E_h[G(t')]\text{ if } t_s\in [T^{w}_{1}, T^{c}_{1})\\
E_h[G(t'')]-E[G(t'')]+E[G(t-t_s)]\\
\text{ otherwise }
\end{array}\numberthis \label{Eftts}
\right.
\]

Combine expression \eqref{ExpectSleep} and \eqref{Eftts}, the expected latency of periodic sleep sensors can be computed.

\end{document}