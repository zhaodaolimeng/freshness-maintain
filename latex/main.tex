%% bare_conf.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

\documentclass[conference]{IEEEtran}

\usepackage{graphicx, url, tikz}
\usepackage{standalone}
\usepackage{amsmath,bm,times,amssymb}
\usepackage{mathtools}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{subcaption}
\usepackage{algorithm,algorithmic}

\usetikzlibrary{datavisualization}
\usetikzlibrary{shapes,arrows,shadows}
\usetikzlibrary{datavisualization.formats.functions}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\RequirePackage[singlelinecheck=off]{caption}

%%%%
\definecolor{mygreen}{RGB}{117,167,117}
\definecolor{myred}{RGB}{255,1,1}
\newcommand\myent[1]{%
  \footnotesize%
  $#1$
}
\colorlet{negro}{black}
\colorlet{gris}{black!70}
\colorlet{rojo}{red!70!black}
\colorlet{rojol}{red}
%%%%

\newtheorem{condition}{Condition}
\newtheorem{assumption}{Assumption}
\newtheorem{colloary}{Colloary}
\newtheorem{theorem}{\bf Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{notation}{Notation}
\newtheorem{definition}{\bf Definition}
\newtheorem{remark}{Remark}

\begin{document}
\title{EasiCrawl: A Sleep-aware Schedule Method \\for Crawling IoT Sensors}

% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{
\IEEEauthorblockN{
	Li Meng\IEEEauthorrefmark{1}\IEEEauthorrefmark{2}, 
	Haiming Chen\IEEEauthorrefmark{1}, 
	Cui Li\IEEEauthorrefmark{1}}
\IEEEauthorblockA{\IEEEauthorrefmark{1}Institute of Computing Technology, Chinese Academy of Sciences}
\IEEEauthorblockA{\IEEEauthorrefmark{2}University of Chinese Academy of Sciences, China}
\IEEEauthorblockA{\{limeng, chenhaiming, lcui\}@ict.ac.cn}
}

% make the title area
\maketitle


\begin{abstract}
	
Search of the Internet of things is an important way for people to exploit useful sensors. Crawling the content of low power IoT sensors is a fundamental step towards building a generic IoT search engine.
However, the crawl of IoT sensors is very different from that of Web. In IoT systems, many low-power sensors may sleep periodically, which results in invalid crawls and unpredictable latency. Besides, the energy consumption of crawl access becomes non-negligible.
As a consequence, the traditional crawling schedule strategy is not suitable for IoT search engine.
We first formulated this periodically sleeping sensors, as a constrained schedule optimization problem, with the objective of minimized latency to crawl information from sensors. 
Then, we developed an a heuristic schedule, named EasiCrawl to get the near optimal expected latency. 
At last, EasiCrawl is evaluated with simulations which show advantages over other greedy-based methods.
A case study is also performed with real world data from Xively, showing that EasiCrawl has lower crawling latency and energy consumption than traditional Web-based search scheduling.

\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction} \label{introduction}

A large number of sensors have been deployed and connected to the Internet to collect physical information, which is provided for the human society to sense and control the physical space in an intelligent way. 
% Importance
The extended Internet with sensors are known as the Internet of Things (IoT). 
With booming number of sensors in IoT, it is inevitable to develop a generic search engine for users and applications to find different kinds of IoT sensors. 
So, it can be expected that a generic IoT search engine can be used in variety scenes to improve the possibility of sharing sensors with others.
In such a way, search of IoT can significantly reduce the redundancy of sensor deployment and facilitate construction of different IoT applications using method like IFTTT\cite{ifttt}.
% More approperate examples needed
More specifically, in smart city applications, public sensors can be found in an ad hoc manner to provide predicates as traffic jam and queuing status. 
Smart home applications need searchable sensors and devices to detect human activities and provide human-centric sensing, which calls for a realtime sensor discovery mechanism. In recent future, unmanned systems like automatic driving systems and quad-copter delivery systems are likely to be heavily dependent on information gathered from nearby sensors or other IoT sensors, which also needs IoT search. 

% Practicability 
The IoT search engine is technically realizable with the help of standardized protocols and semantic descriptions of sensors\cite{Pfisterer2011}. 
Specifically speaking, the descriptions crawled by IoT search engines include not only static contents, but also dynamic records generated by the sensor. The realtime state of a sensor is described by a dynamic description method.
Indeed, many research group are working on dynamic describing methods, such as CoRE\cite{CoREWorkingGroup2012} from IETF and SensorML\cite{botts2007opengis} from OGC. 
Fig.\ref{fig:architecture} gives the architecture of an IoT search system.
In this search system, we suppose every sensors supports IP protocol.
It treat sensors as Web pages, where sensors can automatically update their description files.
IoT crawlers crawl different sensors for these descriptions files, and copy these into a local copy at server end, called repository. The repository is an organized storage, with whom the user queries about IoT sensors can be accomplished.
We assume the user only interest in the events monitored by corresponded sensors, which arrives periodically or just randomly. 
With the help of semantic IoT method, IoT sensors capture the event stream, and continuously store these events, and update their description files, which correspondingly record the state of sensors. 
IoT search engine regularly accesses these IoT descriptions and updates the related repositories. Then, users' query can be responded with the fresh information.

\begin{figure}
	\vspace{0.5em}
	\centering
	\hspace{-3.0em}
	\includestandalone[width=\linewidth]{pic/architecture}
	\captionsetup{justification=centering}
	\caption{Architecture of IoT Search System}
	\vspace{-1.0em}
	\label{fig:architecture}
\end{figure}

% Challenge of IoT search
However, even with dynamic description, the design of IoT search engine is not a trivial task. The main challenge is induced from the instinctive characteristics of IoT. 
Firstly and the most importantly, IoT sensors behaves differently in sleep mode, whereas timely response is a critical performance of search. If a sensor is sleeping, it can neither capture events nor be visited by IoT crawlers. If information are not collected in a right timing, the realtime quality of IoT search will be impacted significantly.
Secondly, most of IoT sensors are low-power device, where the energy consumption is a critical factor. A unnecessary high frequency of visiting a sensor may quickly exhaust battery energy.
Thirdly, IoT sensors is not hyper-linked by their content, so link-based crawl method is infeasible for IoT search engine. 
We use a clip of real world data to illustrate how the instinct of sensors' sleeping impact the crawl quality of IoT search system.
Fig.\ref{fig:smarthome} is a data stream from a temperature sensor located in a smart home environment, who acts as a sensor for air-conditioner control. 
If the value are varying too quickly or the value is out of safe boundary, then the event is logged as an \textbf{interesting event} into the description file.
When users are not at home, these sensors will go into sleep mode. 
We marked all these interesting events out in the figure.
The traditional crawl scheduling method perform badly in this case, because lacking awareness of sensors' sleep. Traditional scheduler will crawl sensor description files periodically, causing invalid access to sleeping sensors, like what is showed at time 30 in Fig.\ref{fig:smarthome}. Bad crawl time cause by delay across the whole sensor' sleeping cycle, at time 210, may also significantly increase latency. 
In this paper, we aim to accelerate IoT search by taking the second and third characteristic of sensors into considerations.
We propose a new server-side scheduling method, EasiCrawl, which can schedule the crawling process with a guarantee of low latency when taking sensors' sleep pattern into consideration.

\begin{figure}
	\centering
	\hspace{-3.0em}
	\includestandalone[width=\linewidth]{pic/smarthome}
	\captionsetup{justification=centering}
	\caption{Temperature Sensor in a Smart Home Environment}
	\vspace{-1.0em}
	\label{fig:smarthome}
\end{figure}

This paper has three contributions.
(1) Although crawler scheduling is a well studied problem, to our best knowledge, our paper is the first to discuss crawling problem for IoT system.
(2) An iterative optimization approach EasiCrawl is proposed for crawler scheduling under different circumstances.
(3) We evaluate EasiCrawl with simulation and real world data crawled from the IoT platform Xively\cite{xively}, which indicates the feasibility of our methods.
Our paper is organized as follows. 
We formalize the optimization problem in section \ref{formulation}, then introduce EasiCrawl in section \ref{easicrawl}. Simulations are performed in section \ref{simulation} and a case study are introduced in section \ref{case_study}. 

\section{Related Work}\label{related_work}

There are lot of off-the-shelf technologies that can be used for IoT search.
We first investigate these network configure protocols used in LAN to automate domestic devices, which have device discovery mechanisms.
UPnP\cite{UPnP}, Bonjour\cite{Bonjour} and other prevalent network configure protocols are actually being used in applications like smart home, etc.
But they are not suitable for IoT applications, mainly because od the power. They are designed for devices with continuous power supply. With low-power sensors which may periodically enter sleep mode, these protocol may fail.
Also, the device description language used by these protocol can not define the realtime state of IoT sensors. So, most of these protocols can not be widely used for an IoT search sense.

Beside these network configure protocols, many IoT search systems with different architectures have been proposed recently.
Due to how the sensor information is gathered, these architectures can mainly be summarized into two categories, push-based and pull-based.
In a push-based system, the integrity of data is a prior.
Sensors periodically sends their data and servers store the data once received. Some of the search requests of push-based systems are running on continuous datastreams, while other system collect data from sensors, and then perform query directly in databases using SQL. 
Technologies like TinyDB\cite{TinyDB} and IoT feeds\cite{Whitehouse2006} take IoT sensors as datasources, and process structured command to query the data stream generated by these sensors. 
This kind of IoT systems are easy to be implemented, but the redundancy of raw sensor data may causes problems in energy and data transitions.
In many applications, not all data is need to be sent and uploaded to server, and this problem seems inevitable when sensors don't know exactly what the user want.
On the other side may be the solution to the requirement match problem.
Pull-based architecture use semantic description, i.e. XML, to describe IoT sensors. Sensors update their description based on data collected, which make the behavior of whom very similar to a web page. This property makes search IoT with a traditional Web search engine possible. For example, Dyser\cite{Dyser} use crawlers and modern indexing methods, along with semantic IoT technology as IoT sensor descriptions. Previous works of semantic IoT\cite{Compton2012} and the propose of CoRE\cite{CoREWorkingGroup2012} devoted to standardize IoT sensor description languages, which indicated the possibility of building a web-style IoT search system. The description files can be accessed by simple retrieve the ./well-know file used in the CoRE framework. 
Although the communication over-head of pull-based architecture is relatively heavy, and when lacking of prior information, most crawlers schedule methods are actually myopic, the scalability make pull-based IoT search architecture very promising.
EasiCrawl are implemented on a pull-based IoT search architecture similar to Dyser\cite{Dyser}, where events captured by IoT sensors are stored in their semantic descriptions.

Crawlers Scheduling is a well studied topic in the field of Web search. Previous works\cite{Cho2000}\cite{Wolf2002}\cite{Challenger2004} analyzed how the crawler strategy affect the freshness of the repository of web pages maintained by web crawlers, which determines the query hit rate and whether this system can return a up-to-date result. 
But in a IoT search system, the sleep behavior of sensors may significantly impact the latency of the crawl of information.
Also, to our best knowledge, no research is find to study the crawl schedule with energy constrained crawl targets.
These two problem make IoT crawler's strategy very different from crawlers in a traditional Web search engine. 

\section{Formulation}\label{formulation}

In this section, we introduce the formulation of crawl schedule problem, along with all the symbols used in the following paper. The follow assumptions are set to simplify sensors' working model.
(1) Events that users care arrived at each IoT sensor in a Poisson manner, where the time interval between any two event follows exponential distribution. 
(2) When sensors are in sleep mode, they can not be accessed by crawlers, and no new events can be captured or stored.
(3) User only cares about the event latency, which is the object function for optimization.
(4) Sensor cannot be accessed too frequently.
(5) During time range $T$, the server has a limited chances to crawl.

EasiCrawl is invoked periodically and then compute a schedule plan for the next $T$ time length.
We also call $T$ the \textbf{time range} of EasiCrawl in the following paper.
Suppose there are totally $N$ sensors that need to be continuously crawled.
For the model simplicity, a discretization method is implied, and finite numbers of time points can be used for crawl.
We set up a discrete time table with a step length $\varepsilon$, each crawl must be preformed at one of these time point in the time table.
Our goal is to find a property \textbf{schedule plan} $\Phi$ so that the latency expectation of all $N$ IoT sensors is minimized. 
The schedule plan consists of two part. The first part is the \textbf{arrangement of crawls} $\hat{n}$ for each sensor $i$, written as $\hat{n}=n_1,n_2,\ldots,n_{N}$. We also use the symbol $\Phi(\hat{n})$ to indicate the schedule plan with arrangement $\hat{n}$.
The second part is the specific \textbf{crawl plan} for each sensors, which is a list of time points for crawling. We denote this list as $\phi_{i}$ for convenience.
In order to model the sleep pattern of each sensor, we use a list of time slots to represent the working duty cycle of sensors, which is called \textbf{sleep plan} for brief. 
The discretization process takes the sleep plan of each sensor as input, and generate discretized time points for each sensor, where time points of sleeping period of sensors are not included.
Sometimes the importance of sensors are different, so we define a factor $\omega_i$ as the expectation weight of sensor $i$. 
Here we use the notation $E_{L}(n_{i})$ as the minimal \textbf{latency expectation} that can be gained by sensor $i$ with totally $n_i$ chance to crawl.
The final object function $E_L(\Phi(\hat{n}))$ is the sum of latency expectation of all sensors which is extended as $\Arrowvert \omega_i E_{L}(n_{i}) \Arrowvert_{1}$.

In addition, this problem follows two constraints. As previously described, server load and IoT sensors' energy consumption are considered. 
During the time range $T$, the access time $n_i$ for sensor $i$ is limited to be accessed less than $\gamma_i$ times. 
For the server-side, we use a similar as the optimal schedule\cite{Wolf2002}, where the crawlers from the server can at most commit $\theta$ crawls during the time range $T$. This models the largest bandwidth of server-side.
With the object function and constraints, we arrived at the following formulation. As discussed above, the solution $\Phi$ contains the optimal crawl arrangement $\hat{n}$ and crawl plan $\phi_i$ for each sensor $i$.
\begin{eqnarray}
\begin{array}{ll}
\min_{\Phi}& E_L(\Phi(\hat{n}))\\
\text{s.t.}
& \sum_{i=1}^{N} n_i \leq\theta\\
&n_i\leq\gamma_i\\
&i=1,\ldots,N
\end{array}\label{OBJ}
\end{eqnarray}

Problem \eqref{OBJ} is a generalized formulation, which does not require for specific sensor model or sleep plan. Due to different requirements between IoT applications, it's not rational to use an universal model to formalize the sleep pattern. For many sensors, the sleep pattern is unknown, while others may have been per-arranged.  
In the follow section, different working model of sensor is discussed. 


\section{Crawling IoT Sensors with EasiCrawl} \label{easicrawl}

We first give out method framework of EasiCrawl, which iteratively improve our solution to achieve a latency optimal scheduling plan. 
Then technical details of subroutines of EasiCrawl is introduced, including the schedule method for periodic or none-periodic sleep sensors. At last, we discuss the strategy for sensors whose sleep plan is previously unknown.

\subsection{Method Framework}
The main idea of EasiCrawl is to arrange crawl chances $\hat{n}$ to the sensors according to their performance in reduction of latency expectation. This arrangement is done in an iterative manner.
Firstly, an original arrangement is made in proportion to an estimated latency, which is the sensors' importance $\omega$ multiple the length of working time.
Then EasiCrawl started to use an iterative method to improve the object function $E_L(\Phi(\hat{n}))$ step by step. 
In each iteration, EasiCrawl searches for a best change in arrangements which can deduce the latency expectation most quickly.
The step length for change is denoted as $\epsilon$.
With these analysis, the minimization problem can be transformed into the following two sub-problems.
First, how to rearranging crawl numbers for each sensor under two constraints? Second, what's the best scheduling strategy for each sensor? 
Once crawl times $n_i$ is chosen, the optimal crawl strategy for a single sensor is determined. Here, every function of $E_L(n)$ is non-decreasing. 
To iteratively improve the global result, we use an approach to change a pair of $n$ at the same time.
First we dispatch the $M$ crawl time to $N$ sensors according to $\omega_i$. Then we start to search sensors for a pair $i$, $j$, where $i$, $j$ are both arrangements. If $i\gets i-1$ and $j\gets j+1$ will cause a largest improvement to the object function compared to the other pairs, then changes of $i$ and $j$ are conformed. This process will be continued until none improvement can be made. 

\begin{algorithm}
	\caption{EasiCrawl Method Framework}
	\label{alg:framework}
	\begin{algorithmic}[1]
		\renewcommand{\algorithmicrequire}{\textbf{Input:}}
		\renewcommand{\algorithmicensure}{\textbf{Output:}}
		\REQUIRE $\mathbf{T}$, $N$, $\theta$, $\epsilon$
		\ENSURE  $\phi$
		\\ 
		\STATE $\hat{n}\gets EvenlyDivide(\theta), LastExpect\gets 0$
		\WHILE {$E_L(\Phi(\hat{n}))-LastExpect>\epsilon$}
		\STATE $BestChange \gets 0, i'\gets 0, j'\gets 0$		
		\FOR {each sensor pair $i,j\in$ sensors}
		\STATE $\hat{n}'\gets n_1,...,n_i+1,...,n_j-1,...,n_N$
		\STATE $c\gets E_L(\Phi(\hat{n})) - E_L(\Phi(\hat{n'}))$
		\IF {$c > BestChange$}
		\STATE $BestChange \gets c, i'\gets i, j'\gets j$
		\ENDIF
		\ENDFOR
		\STATE $\hat{n}\gets n_1,...,n_{i'}+1,...,n_{j'}-1,...,n_N$
		\ENDWHILE 
		\RETURN $\Phi$
	\end{algorithmic} 
\end{algorithm}

The above pseudo code \ref{alg:framework} explain the main process of EasiCrawl. 
The input parameters weight of each sensor and discretization step length are omitted.
Step length is also an input variable for EasiCrawl, here we select value 1 for brief.
In step 1, method $EvenlyDivide$ initialize the crawl arrangements $\hat{n}$. 
Schedule plan $\Phi$ is iteratively improved during steps 2 to 11. 
EasiCrawl search for the best change from Step 3 to 10, and then replaced $\Phi$ with a better one.

Here, we explain why this method can lead to optimal schedule plan.
Indeed, the correctness of EasiCrawl is based on a fact that the latency expectation function for each sensor $E_L(n)$ are convex. 
Then the convexity of the object function $E_L(\Phi(\hat{n}))$ can be proved, which is the sum of $E_L(n_i)$ for each sensor $i$.
Notice that the constrains of are convex, \textbf{problem \eqref{OBJ} is indeed a convex optimization problem}, in which iterative method always points to a optimal solution.
The next step is to prove the convexity of $E_L(n)$.
More specification of sensor's sleep plan is needed for this proof.
First, we use $E_f(t_s,t)$ to represent the latency expectation of a time period, where $t_s$ and $t$ are the start time and end time. If the crawl plan are located at $t_1,t_2,...,t_n$, then latency expectation of a single sensor can be rewrote as $E_L(n) = \sum_{i=1}^{n-1}E_f(t_{i},t_{i+1})$. We use $E[G(t)]$ as an alias for $E_f(t_s,t)$ for simplify.
The convex of latency expectation $E_h[G(t)]$ is proved in the appendix. 
Time periods $[t_i,t_i+1]$ shares a same expectation function $E[G(t)]$. 
The convexity of $E_L(n) = n E_h[G(t/n)]$ can be proofed by the convex preserve operations. 
In this way the convexity of $E_L(n)$ can be proved.
Our result in simulation section also shows a supportive result for these analysis.
In addition, the computation of $E_h[G(t)]$ and $E_f(t_s,t)$, which are both latency expectation of a single senor, is discussed in the appendix.

In the following paper, we first designed a dynamic programming method to compute the crawl plan for non-periodic sleeping sensors. 
This method can return the optimal solution, but the time complexity is somewhat unacceptable for large datasets.
So, we proposed a greedy method as a trade-off for time efficiency. 
All these two methods are used to compute the crawl plan $\phi$ of a single sensor with sleep behavior.

\subsection{Dynamic Programming Method for Optimal Crawl Plan}

In a real world IoT system, sensors' sleep plan make them difficult to crawl in a fix interval. 
Actually, the best crawl plan may be very different from a periodic crawl, especially when the sleep plan is not periodic.
With the previous discretization process, we have a chance to enumerate all the possible crawl plans. The following dynamic programming(DP) method take an advantage of memorized search, which cached the sub-problem to accelerate the enumeration process.

We use \textbf{timeline} to refer the discretized time table for a single sensor, which consist of the working cycle and the sleeping cycle.
The crawl plan can actually be formed as a route planning problem given the following transformation.
Here a directed acyclic graph(DAG) model $G(E,V)$ is used to formalize this problem.
$E$ are the edges of the graph, and $V$ are the vertex. We suppose there are $m$ vertex. 
There are $T$ time points in the timeline which stand for the working cycle. 
Corresponding, there are exactly $T$ vertex in this graph, from number $1$ to number $T$. 
We define $n$ as the count of crawls used, which indicates there are $n-1$ vertex allowed to pass, because the last vertex must be chosen. 
Now we take every points which stand for the working cycle as graph vertex, written as $v_i$. 
For each pair of time points $t_i$ and $t_j$, if $t_i<t_j$, a directed edge $e_{i,j}$ from $v_i$ to $v_j$ is added to the graph. 
A weight value $w_{ij}$ is set to $e_{i,j}$, whose value is $E_f(t_j, t_i)$, is the latency expectation gain of a crawl at time $t_j$ whose previous crawl is at $t_i$.
Our problem is, how to find a path $e_{1,i},\ldots,e_{j,T}$ from $v_1$ to $v_m$, which take exactly $n$ steps, that minimum the sum of all the weights alongside. 
This transform process can be illustrated by Fig.\ref{fig:problemtrans}.
\begin{figure}
	\centering
	\includestandalone[width=\linewidth]{pic/problemtrans}
	\captionsetup{justification=centering}
	\caption{Example of Non-periodic Hibernate Problem Transformation}
	\label{fig:problemtrans}
\end{figure}

With an acceptable scale of time range $T$ and crawl time $n$, this problem can be solved by DP method \ref{alg:dp_min}. 
Solution state with a vertex $v_i$ and the available crawl time $n'$ left can be used as an sub-problem. 
The optimal expectation can be computed by adding weight $w_{i,j}$ with all the expectation known in sub-problems. 
The recursive relationship can be summarized with expression \eqref{dp}.
\[f(v_i, n')=\left\{
\begin{array}{lr}
\min_{k=1}^{i-1}\{f(v_{i-k}, n'-1)+w_{i,i-k}\}\\
\text{if }i\in[1, m-1]\\
0\text{ if }n'=0\text{ and }i=1\\
\infty\text{ otherwise}
\end{array}\numberthis \label{dp}
\right.
\]

$f(v_i,n')$ is the sum of weight of a path that start from $v_1$ and end with $v_i$, with cost exactly $n$ steps. In each step, we go over the  The computation complexity of iterating process is $O(T^2n)$, in which $nT$ sub-problem are computed and each computation will cost $n$ operations. The backtrack method uses $p(v_i, n)$ to get the optimal strategy, which takes at most $n$ steps. As a result, the overall time complexity is still $O(T^2n)$.
\begin{algorithm}
	\caption{DP Method for Crawling Non-periodic Sensors}
	\label{alg:dp_min}
	\begin{algorithmic}[1]
		\renewcommand{\algorithmicrequire}{\textbf{Input:}}
		\renewcommand{\algorithmicensure}{\textbf{Output:}}
		\REQUIRE $G(V,E)$, $n$
		\ENSURE  $t_1,\ldots,t_n$
		\\
		\STATE $f(v_i,n') \gets \infty$, $p(v_i,n')\gets 0$
		\FOR{$n'=1$ to $n-1$}
		\FOR{$i=2$ to $m$}
		\STATE $f(v_i,n')\gets\min_{k=1}^{i-1}\{f(v_{i-k}, n'-1)+w_{i,i-k}\}$
		\STATE $p(v_i,n')\gets$ minimum $v_{i-k}$
		\ENDFOR
		\ENDFOR
		\STATE $t_n\gets T$, $t_i\gets$back trace from $p(v_i, n)$ for $t_i$
		\RETURN $t_1,\ldots,t_n$
	\end{algorithmic}
\end{algorithm}

\subsection{Greedy Method for Periodic Sleep Sensors}

Previous DP method is a generic way to compute the accuracy crawl plan. 
Although it's proven a optimal solution, the computation complexity is relatively high.
Most of the time, there is no need to compute a optimal plan, especially when the computation resource is intensive.
So next, we introduce a greedy crawl planning method.
This method gives a optimal solution when the sensors are none-sleeping. 
It's also a perfect approximate in the other situations.
We start from the continuously working situation. 
An intuition is to evenly distributed the crawling action across the crawling schedule. 
We prove this method actually leads to an optimal value.

\begin{lemma}
\label{evenly}
The optimal crawl plan of an continuously working sensor is to scatter the crawl action over the timeline $t\in [0, T]$ as evenly as possible, where $T$ is a constant.
\end{lemma}
\begin{IEEEproof}
The evenly crawl proposition here can be proved by the convexity of $E_h[G(t)]$ \cite{boyd2004convex}. Here we donate $E_h[G(t)]$ as $f(t)$ for convenience. 
We start the proof with a simple situation where only two crawling are allowed. 
As a result, the whole expectation gain during $[0,T]$, which is marked as $F(0, T)$, is divided into two part, $f(t)$ and $f(T-t)$. With the convexity of $f(t)$, we get $2f(T/2)<f(T/2-\epsilon)+f(T/2+\epsilon)$ for $\forall{\epsilon>0}$. By substituting $F(T)=f(t)+f(T-t)$ into it, we get $F(T/2)<F(T/2+\epsilon)$ for $\forall{\epsilon>0}$, indicating $T/2$ is the minimum point of $F(t)$.
This conclusion can be generalized to a normal situation where multiply crawling are committed with the following induction. 
Now suppose there are $n$ crawling action, among which first $n-1$ action evenly divide the timeline. The time length of $n$-th crawling action does not equal to the adjacent one's, and we claimed this strategy is optimal. For any adjacent pair divided by a single crawling action, if the division is not evenly, we can choose the evenly one as a new strategy, which can lead to a larger $f(t)$ according to the proposition above, which is a contradiction.
\end{IEEEproof}

With Lemma \ref{evenly}, we can get the exact increment of expectation when different crawling times are implied to a same sensor. This can be used to minimize the overall expectation of repository latency, which formalized as $\Arrowvert \phi\mathbf{T} \Arrowvert_{1}$ above.
Here the relationship between crawling time and $E[\phi(n)]$ can be computed simply as follows, where $\phi(n)$ stands for the optimal strategy with $n$ crawls, and $T$ is the time period within consideration. 
\begin{equation}
E[\phi(n)]=nE[G(\frac{T}{n})]\label{NonHib}
\end{equation}

Function \eqref{NonHib} can be rewrite to an close-form expression as $T(T\lambda/n-1+e^{-T\lambda/n})$, which is convex. with this function, the minimum expectation can be calculated directly.

Here we also study the relationship between $n$ and the minimum latency expectation, but try to be more pragmatic.
With the optimal strategy $\phi_i(n)$ for sensor $i$ which is only related to the crawling time $n$, we can substitute functions \eqref{Eftts} and \eqref{ExpectCont} into \eqref{OBJ} to get the latency expectation of non-hibernation version. 
But with hibernation, at some time points the sensors can not be accessed, resulting in a complex piecewise expectation function, which makes the relationship between $n$ and $\phi_i(n)$ unclear and completely theoretical approach likely impossible.
Here we focus on the situation when $T_w<T_h$ and $n<T/\Delta T$. Applications which hold $T_w\leq T_h$ is rare, while if $n<T/\Delta T$ is just the same as the push situation we talked before.
A concrete formulation extends from \ref{OBJ} is as follows, where $\mathbf{t}$ is the set of target time sequence that a crawl action can be scheduled to, and $k\leq 0$.
Here an intuition of the problem is to arrange the crawls at the end of a work cycle, we show this intuition is indeed an optimal strategy when the crawl interval is an integral multiple of duty cycle $\Delta T$. We uses the same symbols as the previous.

\begin{lemma}
\label{intopt}
If the time interval of an adjacent pair of crawls at $t$ and $t+\Delta t$ is an integral multiple of duty cycle, then the optimal crawl time $t$ follows $t\bmod \Delta T=T_w$.
\end{lemma}

\begin{IEEEproof}
The difference between expectations with two different time intervals, which have the different offset and have the same length multiple of duty cycle, can be written as $E_f(t, t_s)-E_f(t+\Delta t, t_s+\Delta t)$. 
The expansion of $E_f(t,t_s)$ is $E_h[G(t'')]-E[G(t)'']+E[G(t-t_s)]$, notice that $E_h[G(t'')]$ has a lower change rate compared with $E[G(t)]$, whose coefficient is $T_w/\Delta T$ instead of $1$. This indicates that the previous difference is a non-increasing function, which the optimal is the minimum when $\Delta t =T_w$.
\end{IEEEproof}


% How about the general situation? The same
The solution of the general situation is very similar to the previous when both $t$ and $t+\Delta t$ locate in working cycles. However, when crawl $t+\Delta t$  locate in a hibernate cycle the expectation will become $0$. Also, the result will become opposite when $t$ locate in a hibernate cycle while $t+\Delta t$ locate in a working one. In this situation, the expectation will become larger with more time of work cycle included into the expectation area. 
With these observations, we can design a strategy that in some circumstances, the optimal solution can be retrieved.
The result can be computed in one pass, so the complexity of this approach is $O(n)$, where $n$ is the size of crawl times.

\section{Simulation and Evaluation} \label{simulation}

In the evaluation section, we first test the performance of EasiCrawl under different parameters, and confirm the effectiveness with real events sequence. Latency expectation which are previously used as a optimization object are taken as measurement.
Then, we carry out a simulation comparing efficiency and computation complexity of 3 different crawl methods implemented in EasiCrawl. Here the latency sum of generated events and executing duration are considered as measurements.
Sensor scales $N$, total crawl limit $\theta$, discretization step length $\epsilon$, and time range $T$ of scheduling are tested during the following simulations. We use a random schedule method, called RandomCrawl, to illustrate the effectiveness of our method. RandomCrawl use the same method as EasiCrawl to initialize the arrangement of crawls to different sensors. Then, for each sensor, the crawl plan are set randomly, the last timestamp are chosen to ensure all the event are captured. In addition, the time unit of these simulation are all minutes, and we take 1 min as the minimum unit that can be used for scheduling.
All these simulation are implemented on a PC with MATLAB.
\begin{figure*}
	\minipage{0.32\textwidth}
		\includestandalone[width=\linewidth]{pic/test1_sensorscale}
		\captionsetup{justification=centering}
		\caption{Latency Expectation With Different Sensor Scales}
		\label{fig:test1_sensorscale}
	\endminipage\hfill
	\minipage{0.32\textwidth}
		\includestandalone[width=\linewidth]{pic/test2_totalcrawl}
		\captionsetup{justification=centering}
		\caption{Latency Expectation With Different Crawls}
		\label{fig:test2_totalcrawl}
	\endminipage\hfill
	\minipage{0.32\textwidth}%
		\includestandalone[width=\linewidth]{pic/test3_discretestep}
		\captionsetup{justification=centering}
		\caption{Latency Expectation With \\Different Discretization Step}
		\label{fig:test3_discretestep}
	\endminipage
	
	\minipage{0.32\textwidth}
		\includestandalone[width=\linewidth]{pic/test4_timerange}
		\captionsetup{justification=centering}
		\caption{Latency Expectation With \\Different Total Time Range}
		\label{fig:test4_timerange}
	\endminipage\hfill
	\minipage{0.32\textwidth}
		\includestandalone[width=\linewidth]{pic/test5_sensortype}
		\captionsetup{justification=centering}
		\caption{Latency Expectation With \\Different Sensor Types}
		\label{fig:test5_sensortype}
	\endminipage\hfill
	\minipage{0.32\textwidth}%
		\includestandalone[width=\linewidth]{pic/test6_convergespeed}
		\captionsetup{justification=centering}
		\caption{Converge Speed under \\Different Crawl Times}
		\label{fig:test6_convergespeed}
	\endminipage	
\end{figure*}

\subsection{Sensor Scales $N$}
We first study the impact of sensor scale. We mainly test the dynamic programming schedule method of single sensor in this simulation. All the sleep plan of sensors are previously known and non-periodic, actually we use a randomized method to generate the sleep plan for each sensor. The total crawl limit $\theta$ is set as the multiple of sensor scale, varies from 10 to 200. The Poisson parameter $\lambda$ of each sensor is randomly chosen from 0 to 1. The schedule time range $T$ is set as 200 time units. In each improvement iteration, the crawl time arrangement $n_i$ for different sensors are changed by the step length of 1. For each sensor scale, we repeated the simulation for 5 times, and compute the average, minimum and maximum value of latency expectations. Simulation result is plot in Fig. \ref{fig:test1_sensorscale}.


Simulation shows a promising result. The latency expectation of EasiCrawl is far below the random one, even take error bar into consideration. This is mainly because the theoretical optimal property of EasiCrawl when given a sequence of discretized time nodes. 

\subsection{Total Crawl Times $\theta$}

The total number of crawls which the server can at most start is an important constraint for EasiCrawl. In this test, EasiCrawl worked on sensor scale of 10, crawling time range of 200, and discrete step length of 2, where minute is the unit. We use RandomCrawl as a reference, and test 40 different crawl limits on EasiCrawl and RandomCrawl, ranging from 10 to 50, which show a result as Fig.\ref{fig:test2_totalcrawl}.
The result first show the advantages of EasiCrawl against RandomCrawl. More important, we notice there's a submodular trends for total crawls, which means the gain of server side crawl ability pay less when limits grow larger, and this result is theoretically reasonable. This suggest us to find a proper trade-off of server cost and schedule efficiency.

\subsection{Discretization Step Length $\epsilon$}

Time table of sleep plans should be discretized before starting scheduling. There is a trade off between the computation time and final accuracy. 
We use a simulation with 10 sensors and a time range $T$ of 500, to test the relationship between time complexity and expectation accuracy. All the sensors are set to be scheduled with dynamic programming based method. The total crawl count $\theta$ is set to 100 times.
We carried on 20 groups of test, where the discretization step length $\epsilon$ ranges from 1 to 20.


Fig.\ref{fig:test3_discretestep} shows a positive result to our analysis of the dynamic programming crawl method used in EasiCrawl. While latency expectation increase about 400 units, the computation time decreased from 1 seconds to below 0.1 seconds. This result fits the theoretical analysis, where for scheduling each sensor, the time complexity is $O(T^{2}n)$, $n$ is the crawl limit.
When $\epsilon$ is relatively small, the number $M$ of total time nodes which can used to crawl is large, which leads to a higher computation cost.

\subsection{Time Range $T$}

We use the same experiment settings as previous. 10 simulated sensors are used, the total crawl sum $\theta$ is 30. For time range $T$ varies from 50 to 1000, a new time table is generated in each iteration. The time range $T$ which EasiCrawl schedules on has a similar effect as the discretization step $\epsilon$. 
The computation time showed in Fig.\ref{fig:test4_timerange} is very close to the analysis result, which is quadratic. The expectation share a similar shape because the distance between the distribution and the end time $t$ can also be estimated as a quadratic function.

\subsection{Senors' Types}

In the previous evaluations, the DP(dynamic programming) methods are used for finding the best schedule for each sensors. In this test, both the greedy method and dynamic programming are both tested. 
Tests are performed for 10 times with different sensor scale varies from 10 to 100. 4 different group of sensors are used for each test. Sensors in the first group are all scheduled with DP method. In the second group, half of the sensors are, while the other half use greedy based method. The third group used pure greedy based method. We also use the RandomCrawl method as a reference, which is the fourth group.


Result are showed in Fig.\ref{fig:test5_sensortype}. As our expect, DP method performs the best. With the share of greedy method grows, the latency expectation become larger. When all the sensors use greedy method for scheduling, the latency expectation is the worst, but still much better than that of RandomCrawl.

\subsection{Speed of Convergence}
The speed of convergence directly affect the efficiency of the total computation cost. Here we set up a test with two different total crawl times $\theta$, to verify the iteratively optimize method. The time range $T$ here is chose to be 400 time unit. We separately perform this test, first use a total crawl limit $\theta$ of 50, then 100. The sleep plan of sensors and other parameters are the same. The total iterations used before reaching convergence is collected.
Fig.\ref{fig:test6_convergespeed} is the final statistics, where the iteration steps for different experiment setting is close to Gaussian distribution. Apparently, when the number of total crawls grow bigger, a larger mean value is expected.

\section{Case Study With Xively} \label{case_study}

Xively is a platform which integrates IoT sensors. With thousands of sensors which can simply accessed with ip protocols, Xively become a ideal platform for our crawl method.


We built a mini IoT search system to test our crawl scheduling method EasiCrawl, where we continuously monitored several datastream of 3 airqualityegg\cite{airegg} sensors located in London and Tokyo as an example for our case study. 
Practically, what we mean by accessing the sensors is actually accomplished by query the Xively database which hold the up-to-date sensor information. This makes no different for us than directly accessing a real sensor. One problem here is that most raw sensors on Xively don't support semantic senor description functions. So we add a describing layer, which translate raw data streams to event sequence. A sudden raise in temperature or violation of radiation limit are taken as events, and will be stored as a item of description, which is the target to crawl.
\begin{figure}
	\centering
	\includestandalone[width=\linewidth]{pic/xivelycasestudy}
	\captionsetup{justification=centering}
	\caption{Crawl Plan Generated by EasiCrawl for \\
		Data from Xively}
	\label{fig:xivelycasestudy}
\end{figure}


In this case, the parameter for Poission process of the arriving event sequence are estimated previously.
Fig.\ref{fig:xivelycasestudy} is the raw sensor data of 4 different data streams, including CO, Humidity, NO\textsubscript{2}, along with the corresponding scheduling result. The circle marks are the event captured and recorded which users may care. Here, we consider the high level of CO, NO\textsubscript{2}, are danger events, and the quick raise in temperature or quick changes in humidity may be the miss use of electrical equipments, which should be also recorded. We also consider the temperature changes not interesting, only to test the effectiveness of scheduling non-periodic sensors.
The triangle marks are the actual crawls. EasiCrawl can always hold a low latency level, which is 2 to 3 times lower than that of RandomCrawl.


With the total working on the dataset of 2 weeks, EasiCrawl can achieve a total ... ???


\section{Conclusion} \label{conclusion}

In this paper, we introduce EasiCrawl, a crawl scheduling strategy which runs an iteratively improving method. EasiCrawl can be used with different sensors, which is ideal to be used in IoT search system. Effectiveness of EasiCrawl is also showed in simulations and a case study with Xively. 
The drawbacks of EasiCrawl lies in the blindness when crawling sensors whose sleep plan is unknown. 
Also the fixed step length for improve arrangements, which is not a effect way for optimizing, needs to be improved.

\section*{Acknowledgment}


The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi


\bibliographystyle{unsrt}%{IEEEtran}%{unsrt}
\bibliography{reference}


\appendices
\section{Analysis of Latency Expectation}

\subsection{Expectation Without Sleep Mode}
Although previous work\cite{Cho2000} give out a similar result, we introduce a simpler way for deducing $E_f(t_s, t)$ when sensor works without sleep. For simplification, suppose the IoT sensor crawled start working at time $0$. The total freshness latency when crawl at time $t$ can be considered as an random variable $G(t)$, so our goal can be rewritten to compute the expectation $G(t)$, written as $E[G(t)]$. 
Notice $E[G(t)]$ can be divide into different parts due to the count of events $U_i$ happen during $[0, t)$, then $E[G(t)]$ can be written as:
\begin{equation}
E[G(t)] = \sum_{n=1}^{\infty} E[G(t)|N(t)=n]P(N(t)=n) \label{EG}
\end{equation}

$P(N(t)=n)$ is the probability of having $n$ events during $[0,t)$, which follows Poisson Distribution, holding $P(N(t)=n)=e^{-\lambda t}{(\lambda t)^n}/{n!}$ . $E[G(t)|N(t)=n]$ can be resolved as $\frac{n}{\lambda}(t\lambda+e^{-\lambda t_m}-1)$. $t_{m}$ stands for the happened time of the $m$-th event. Taking \eqref{EG} , we get the close form solution of the expectation as equation \eqref{ExpectCont}.
\begin{equation}
E[G(t)] = (e^{-t\lambda}-1+t \lambda)t \numberthis \label{ExpectCont}
\end{equation}

\subsection{Expectation With Sleep Mode}
For the sleep situation, sensor starts to work at time $t_s$, and begin to work with a sleep plan.
Here we mark $T^{c}_{i}$ as the $i$-th working cycle. $T^{c}_{i} = T^{w}_{i}+T^{s}_{i}$, where $T^{w}_{i}$ is the $i$-th working cycle and $T^{w}$ is the $i$-th sleeping cycle. 
It's hard to directly deduce an expectation for a Poisson process whose timeline is break up. However, with equation \eqref{ExpectCont}, we can compute the $E_f(t, t_s)$ by subtract the expectation cause by the previous crawls from $E[G(t)]$. In this recursive way, the expectation can be calculated. Here $E_s[G(t)]$ stands for the expectation to crawl a sleep enabled sensor at time $t$, $E[G(t)]$ still stands for the expectation if this particular sensor doesn't sleep.
Start time $t_s=0$, $E_f(t_s,t)$ can be written as $E_s[G(t)]$.Suppose time $t$ locates in an working period, written as $t \in [t_s + \sum_{i=1}^{k} T^{c}_{i}, t_s + \sum_{i=1}^{k} T^{c}_{i} + T^{w}_{i}]$, where $k\in\mathbb{N}$. This assumption is based on the fact that, any crawl at a sleeping cycle can be bring forward to last work cycle for a better latency expectation.
\begin{align*}
E_h[G(t)] & = E[G(t-T^{L}(K))] + \\
	& \sum_{i=1}^{K}(E[G(t-T^{L}(K-i))]-\\
	& E[G(t-T^{L}(K-i)-T^{w}_{K-i})]) \numberthis \label{ExpectSleep}
\end{align*}

Here $T^{L}(K) = \sum_{j=1}^{K} T^{c}_{j}$, which is time duration of first $K$ cycles, and $K$ is the count of cycle $T^{C}$. 
For any time $t$ in a sensor working cycle, by summing those expectations before $t$, the overall expectation of latency can be achieved when take sensor sleep cycle into consideration. The first factor of \eqref{ExpectSleep} is the expectation of last working cycle, and the sum is the expectation of the previous working cycle.


Expression \eqref{ExpectSleep} is too complex for practical use, so we figure out a way to simplify this. 
We assume $t\lambda$ is relatively large, which means in a work cycle multiply events are trends to be captured.
Notice that if $t\rightarrow \infty$, $e^{-t\lambda}-1+t \lambda \rightarrow 0$, which means expectation \eqref{ExpectCont} can be approximated by a quadratic expression $t^2\lambda$.
With this observation, the expectation gain of a previous work cycle can be approximated using $E[G(t)]-E[G(t-\delta)] = \delta (2 t - \delta)\lambda$, which is a linear function of $t$. Here $\delta$ is the length of working cycle.
This indicates that latency expectation of a sleep enabled sensor can actually be approximated by a piecewise linear function.
According to equation \eqref{ExpectSleep}, each segment of the approximate function is the sum of different non-decrease linear expressions.
By taking approximation of $E[G(t)]-E[G(t-\delta)]$ into \eqref{ExpectSleep}, an approximation of $E_h[G(t)]$ is achieved as the follows.
\begin{align*}
E_h[G(t)] = (t-T^{L}(K))^2\lambda + \sum_{i=1}^{K} (2 t - T^{w}_{K-i})T^{w}_{K-i}\lambda
\numberthis 
\label{ExpectSleepApprox}
\end{align*}

\subsection{Convexity of $E_h[G(t)]$}
Expression\eqref{ExpectSleepApprox} is a simplified form of $E_h[G(t)]$, and it's direct to show this expression is continuous and derivable. Here we try to compute the one-order derivative on $t$.
\[\frac{\partial E_h[G(t)]}{\partial k} = \left\{
	\begin{array}{lr}
	c_i+\gamma_i t & \text{if }t \in [T^{L}_{i}, T^{L}_{i}+T^{w}]\\
	c_i & \text{if }t \in [T^{L}_{i}+T^{w}, T^{L}_{i+1}]
	\end{array}	\numberthis \label{ExpectSleepAnother}
	\right.
\]

The result is a continuous linear piecewise function, where $c_i$ and $\gamma_i$ are constraints. The meaning of expression \eqref{ExpectSleepAnother} is actually the gain speed of latency expectation. With expression \eqref{ExpectSleepAnother}, the second order derivation $\nabla^{2} E_h[G(t)]\geq0$ in its domain, and the convexity of function \eqref{ExpectSleepApprox} is proofed\cite{boyd2004convex}. 


\subsection{Expectation of Arbitrary Period}
Previous analysis computes the expectation with the start time at $0$. The problem of computing $E_f(t,t_s)$ if $t\ne0$, can be solved given the previous analysis. This problem be divided into two situation according to the value of $t_s$.
Assume that $t_s\in [0,T^{c}_{1}]$, other case can be transformed to this by shifting $t_s$ and $t$.
If $t_s$ is in a sleeping cycle, then expectation can just be calculated from the next work cycle.
Otherwise, if $t_s$ is in a working cycle, the expectation can be estimate by $E_h[G(t)]$ which start at $0$ subtract the latency expectation cause by the events arriving at time $[0, t_s)$.

We define $t'=t-(t_s\bmod T^{c}_{1})$ and $t''=t-T^{c}_{1}$ for convenience, where $T^{c}_{1}$ is the length of first cycle.
Here the minuend means the expectation at time point $t$ of latency during time period $[0, t_s]$.
If $t_s$ is at a hibernate cycle, then the result expectation is just as the one which $t_s$ equals next duty cycle, which is $E_h[G(t')]$, which can be written as follows.
\[E_f(t,t_s)=\left\{
    \begin{array}{lr}
    E_h[G(t')]\text{ if } t_s\in [T^{w}_{1}, T^{c}_{1})\\
    E_h[G(t'')]-E[G(t'')]+E[G(t-t_s)]\\
    \text{ otherwise }
    \end{array}\numberthis \label{Eftts}
    \right.
\]

Combine expression \eqref{ExpectSleep} and \eqref{Eftts}, the expectation of latency of periodic sleep sensors can be computed.

\end{document}